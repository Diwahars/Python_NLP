{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\puttuk1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained word vectors from https://nlp.stanford.edu/projects/glove/...\n",
      "Loaded Common Crawl (840B, 300D) word vectors\n"
     ]
    }
   ],
   "source": [
    "def load_glove_cc():\n",
    "\n",
    "    word2vec = {}\n",
    "    with open(r'C:\\Users\\puttuk1\\Downloads\\glove.840B.300d.txt', 'r', encoding='utf8') as f:\n",
    "        for line in islice(f, 0, None):\n",
    "            line = line.strip().split(' ')\n",
    "            if len(line) > 300:\n",
    "                words = line[:-300]\n",
    "                vec = [float(x) for x in line[-300:]]\n",
    "                for word in words:\n",
    "                    word = word.lower()\n",
    "                    word = re.sub('[^a-z0-9]', '', word)\n",
    "                    if word:\n",
    "                        word2vec[word] = vec\n",
    "    return word2vec\n",
    "\n",
    "print('Loading pre-trained word vectors from https://nlp.stanford.edu/projects/glove/...')\n",
    "word2vec = load_glove_cc()\n",
    "print('Loaded Common Crawl (840B, 300D) word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Quora question pairs...\n",
      "Read 404279 samples\n"
     ]
    }
   ],
   "source": [
    "print('Reading Quora question pairs...')\n",
    "samples = []\n",
    "with open(r'C:\\Users\\puttuk1\\Downloads\\quora_duplicate_questions.tsv', 'r', encoding='utf8') as f:\n",
    "    for line in islice(f, 1, None):\n",
    "        line = line.strip().split('\\t')\n",
    "        if len(line) == 6:\n",
    "            samples.append(line[3:])            \n",
    "print('Read ' + str(len(samples)) + ' samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features for question pairs...\n",
      "1. ratio of the difference in token sizes to the mean token size\n",
      "2. ratio of the count of common tokens to the mean token size\n",
      "3. ratio of the count of common tokens (non-stop words) to the mean token size (non-stop words)\n",
      "4. flag represnting the equality of the first token\n",
      "5. flag represnting the equality of the last token\n",
      "6. cosine similarity between the question pair\n",
      "...\n",
      "Stored features for question pairs\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    x = text.lower()\n",
    "    x = re.sub(\"[^a-z0-9']\", \" \", x)\n",
    "    x = re.sub(\"([0-9]+)000000000\", \"billion\", x)\n",
    "    x = re.sub(\"([0-9]+)000000\", \"million\", x)\n",
    "    x = re.sub(\"([0-9]+)000\", \"thousand\", x)\n",
    "    x = x.replace(\"'m\", \" am\")\\\n",
    "        .replace(\"'s\", \" is\")\\\n",
    "        .replace(\"'ll\", \" will\")\\\n",
    "        .replace(\"'re\", \" are\")\\\n",
    "        .replace(\"'ve\", \" have\")\\\n",
    "        .replace(\"can't\", \"can not\")\\\n",
    "        .replace(\"won't\", \"will not\")\\\n",
    "        .replace(\"n't\", \" not\")\\\n",
    "        .replace(\"'\", \" \")\n",
    "    x = re.sub(\" +\", \" \", x)\n",
    "    return x\n",
    "\n",
    "def compute_similarity(q1, q2, w2v):\n",
    "\n",
    "    dflt = [0 for i in range(300)]\n",
    "    v1 = dflt\n",
    "    for x in q1:\n",
    "        v = w2v.get(x, dflt)\n",
    "        v1 = [max([v1[i], v[i]]) for i in range(len(v))]\n",
    "    v2 = dflt\n",
    "    for x in q2:\n",
    "        v = w2v.get(x, dflt)\n",
    "        v2 = [max([v2[i], v[i]]) for i in range(len(v))]\n",
    "    dist = cosine_similarity([v1], [v2])[0][0]\n",
    "    return dist\n",
    "\n",
    "print('Generating features for question pairs...')\n",
    "print('1. ratio of the difference in token sizes to the mean token size')\n",
    "print('2. ratio of the count of common tokens to the mean token size')\n",
    "print('3. ratio of the count of common tokens (non-stop words) to the mean token size (non-stop words)')\n",
    "print('4. flag represnting the equality of the first token')\n",
    "print('5. flag represnting the equality of the last token')\n",
    "print('6. cosine similarity between the question pair')\n",
    "print('...')\n",
    "features = []\n",
    "for line in samples:\n",
    "    q1 = preprocess(line[0]).split(' ')\n",
    "    q2 = preprocess(line[1]).split(' ')\n",
    "    q1_non_stop = [x for x in q1 if x not in stop_words]\n",
    "    q2_non_stop = [x for x in q2 if x not in stop_words]\n",
    "    f1 = float(len(set(q1)) + len(set(q2))) / 2\n",
    "    f2 = float(len(set(q1_non_stop)) + len(set(q2_non_stop))) / 2\n",
    "    f3 = abs(len(set(q1)) - len(set(q2))) / f1\n",
    "    f4 = len(set(q1).intersection(set(q2))) / f1\n",
    "    f5 = len(set(q1_non_stop).intersection(set(q2_non_stop))) / f2\n",
    "    f6 = int(q1[0] == q2[0])\n",
    "    f7 = int(q1[-1] == q2[-1])\n",
    "    f8 = compute_similarity(q1, q2, word2vec)\n",
    "    x = ','.join(list(map(str, [f3, f4, f5, f6, f7, f8])))\n",
    "    y = line[2]\n",
    "    features.append(x + ',' + y)\n",
    "    \n",
    "with open(r'C:\\Users\\puttuk1\\Downloads\\quora_feature_space.csv', 'w') as f:\n",
    "    for line in features:\n",
    "        f.write(line + '\\n')\n",
    "print('Stored features for question pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoostingClassifier...\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2778            1.55m\n",
      "         2           1.2424            1.25m\n",
      "         3           1.2133            1.04m\n",
      "         4           1.1880           55.33s\n",
      "         5           1.1664           52.78s\n",
      "         6           1.1478           52.11s\n",
      "         7           1.1321           51.49s\n",
      "         8           1.1183           51.52s\n",
      "         9           1.1060           53.40s\n",
      "        10           1.0953           54.53s\n",
      "        20           1.0372           59.34s\n",
      "        30           1.0166           57.81s\n",
      "        40           1.0084           53.26s\n",
      "        50           1.0037           45.68s\n",
      "        60           1.0006           36.86s\n",
      "        70           0.9986           26.28s\n",
      "        80           0.9966           16.53s\n",
      "        90           0.9952            7.86s\n",
      "       100           0.9935            0.00s\n",
      "Training score: 0.711197142857\n",
      "Test score: 0.712227564988\n"
     ]
    }
   ],
   "source": [
    "X, Y = [], []\n",
    "with open(r'C:\\Users\\puttuk1\\Downloads\\quora_feature_space.csv', 'r') as f:\n",
    "    for line in islice(f, 0, None):\n",
    "        line = line.strip().split(',')\n",
    "        X.append(list(map(float, line[:-1])))\n",
    "        Y.append(int(line[-1]))\n",
    "\n",
    "print('Training GradientBoostingClassifier...')\n",
    "x_train, y_train, x_test, y_test = X[:350000], Y[:350000], X[350000:], Y[350000:]\n",
    "clf = GradientBoostingClassifier(verbose=1).fit(x_train, y_train)\n",
    "score = clf.score(x_train, y_train)\n",
    "print('Training score: ' + str(score))\n",
    "score = clf.score(x_test, y_test)\n",
    "print('Test score: ' + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
